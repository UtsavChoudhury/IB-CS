Large Language Models (LLMs) are trained on vast datasets that include publicly available source code from open-source repositories (like GitHub), programming tutorials, documentation, technical forums, Q&A sites (such as Stack Overflow), and other web resources. The data covers many programming languages, libraries, and frameworks to help the model learn code syntax, patterns, and best practices. Proprietary (privately owned) or private code is not used unless explicit permission is granted.